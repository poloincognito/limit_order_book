{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8b46c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Input, Conv1D, LSTM, Dense, Reshape, Concatenate, Attention, Reshape, Input, Dropout, MultiHeadAttention\n",
    "from keras.models import Model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd9ab35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('limit_order_book/truncated_LOB_data_BTC_USD_COINBASE.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93636ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finder_of_fulfilment(df, column):\n",
    "    counter = []\n",
    "    for i in range(len(df)):\n",
    "        num = df[column].iloc[i]\n",
    "        arr = df[df[column]>num][column].index\n",
    "        counter.append(df['timestamp'].iloc[arr[arr>i][0]] if len(arr[arr>i]) else -1)\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415cf8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['counter'] = finder_of_fulfilment(df, 'ask_prices_0')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e707e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['counter'] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ecd499",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['counter'] = pd.to_datetime(df['counter'])\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['exec_time'] = (df['counter'] - df['timestamp']).dt.total_seconds().astype(int)\n",
    "df = df.iloc[0:-140]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f85fa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make some features and drop useless columns \n",
    "df.drop(columns = ['timestamp', 'counter'], inplace=True)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "df['vol_imbalance'] = (df['bid_quantity_0'] - df['ask_quantity_0'])/(df['bid_quantity_0'] + df['ask_quantity_0'])\n",
    "df['microprice'] = ((df['bid_prices_0']*df['bid_quantity_0']+df['ask_prices_0']*df['ask_quantity_0'])\n",
    "                    /(df['bid_quantity_0'] + df['ask_quantity_0']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be901f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(10,6))\n",
    "plt.plot(df['microprice'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268a1854",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['exec_time'])\n",
    "Y = df['exec_time']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fe5e5e",
   "metadata": {},
   "source": [
    "# Simplified Staff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7949be",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X.shape[1:]  # Assuming X is your data\n",
    "steps = 1  # You need to determine the appropriate number of steps based on your data\n",
    "new_input_shape = (input_shape[0], steps)\n",
    "encoder_input = Input(shape=input_shape)\n",
    "encoder_reshape = Reshape(new_input_shape)(encoder_input)\n",
    "encoder_conv1 = Conv1D(32, kernel_size=2, activation='relu')(encoder_reshape)\n",
    "encoder_lstm = LSTM(32, activation='relu')(encoder_conv1)\n",
    "latent_dim = 32\n",
    "encoder_output = Dense(latent_dim)(encoder_lstm)\n",
    "\n",
    "encoder_model = Model(encoder_input, encoder_output)\n",
    "\n",
    "# Define the decoder\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "decoder_dense1 = Dense(32, activation='relu')(decoder_input)\n",
    "decoder_reshape = Reshape((1, 32))(decoder_dense1)\n",
    "decoder_conv1 = Conv1D(32, kernel_size=3, activation='relu', padding='same')(decoder_reshape)\n",
    "output_dim = 32\n",
    "decoder_output = Dense(output_dim, activation='sigmoid')(decoder_conv1)\n",
    "\n",
    "decoder_model = Model(decoder_input, decoder_output)\n",
    "\n",
    "# Combine the encoder and decoder into an autoencoder\n",
    "autoencoder_input = Input(shape=input_shape)\n",
    "encoded = encoder_model(autoencoder_input)\n",
    "decoded = decoder_model(encoded)\n",
    "autoencoder_model = Model(autoencoder_input, decoded)\n",
    "\n",
    "# Compile the model\n",
    "autoencoder_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "autoencoder_model.fit(X, Y, epochs=3, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294800d1",
   "metadata": {},
   "source": [
    "# some experiements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf914664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DCC layer (updated version)\n",
    "class DilatedCausalConvolution(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, dilation_rate):\n",
    "        super().__init__()\n",
    "        self.query_conv = layers.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                                        dilation_rate=dilation_rate, padding='causal', activation='relu')\n",
    "        self.key_conv = layers.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                                      dilation_rate=dilation_rate, padding='causal', activation='relu')\n",
    "        self.value_conv = layers.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                                        dilation_rate=dilation_rate, padding='causal', activation='relu')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query = self.query_conv(inputs)\n",
    "        key = self.key_conv(inputs)\n",
    "        value = self.value_conv(inputs)\n",
    "        return query, key, value\n",
    "\n",
    "# Define the Transformer block (updated version)\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, num_heads, d_model):\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.dense_proj = layers.Dense(d_model, activation='relu')\n",
    "\n",
    "    def call(self, query, key, value):\n",
    "        attn_output = self.multi_head_attention(query, key, value)\n",
    "        proj_output = self.dense_proj(attn_output)\n",
    "        return proj_output\n",
    "\n",
    "# Define the encoder using the DCC and Transformer block (updated version)\n",
    "def create_encoder(input_shape, filters, kernel_size, dilation_rate, num_heads, d_model):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    dcc_layer = DilatedCausalConvolution(filters=filters, kernel_size=kernel_size, dilation_rate=dilation_rate)\n",
    "    query, key, value = dcc_layer(inputs)\n",
    "    transformer_block = TransformerBlock(num_heads=num_heads, d_model=d_model)\n",
    "    transformer_output = transformer_block(query, key, value)\n",
    "    model = models.Model(inputs=inputs, outputs=transformer_output)\n",
    "    return model\n",
    "\n",
    "# Define the monotonic decoder (simplified version)\n",
    "def create_monotonic_decoder(input_shape, output_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # The decoder would have constraints to ensure monotonicity, omitted here for simplicity\n",
    "    outputs = layers.Dense(output_shape, activation='sigmoid')(inputs)\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Combine the encoder and decoder to create the full model (updated version)\n",
    "def create_full_model(input_shape, filters, kernel_size, dilation_rate, num_heads, d_model, output_shape):\n",
    "    encoder_inputs = layers.Input(shape=input_shape)\n",
    "    encoder = create_encoder(input_shape, filters, kernel_size, dilation_rate, num_heads, d_model)\n",
    "    encoder_output = encoder(encoder_inputs)\n",
    "    \n",
    "    # Assuming the encoder_output has shape (batch_size, seq_length, d_model)\n",
    "    # If the next layer expects a flat vector, we can flatten the encoder output\n",
    "    # Otherwise, adjust the shape according to the requirements of your decoder\n",
    "    flattened_output = layers.Flatten()(encoder_output)\n",
    "    \n",
    "    decoder = create_monotonic_decoder(flattened_output.shape[1:], output_shape)\n",
    "    decoder_output = decoder(flattened_output)\n",
    "    \n",
    "    full_model = models.Model(inputs=encoder_inputs, outputs=decoder_output)\n",
    "    return full_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e114013f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand dimensions of X to add the channel dimension if needed\n",
    "X_expanded = np.expand_dims(X, axis=-1)  # Shape becomes (99601, 22, 1)\n",
    "\n",
    "# Define model parameters\n",
    "input_shape = (22, 1)  # 22 timesteps and 1 feature per timestep\n",
    "filters = 64\n",
    "kernel_size = 3\n",
    "dilation_rate = 1\n",
    "num_heads = 8\n",
    "d_model = 64\n",
    "output_shape = 1\n",
    "\n",
    "# Create the full model\n",
    "model = create_full_model(input_shape, filters, kernel_size, dilation_rate, num_heads, d_model, output_shape)\n",
    "\n",
    "# Compile the model (define loss and optimizer)\n",
    "model.compile(optimizer='adam', loss='MSE')\n",
    "\n",
    "# Fit the model with the expanded input data\n",
    "model.fit(X_expanded, Y, epochs=10, batch_size=32)\n",
    "\n",
    "# Print the model summary to verify\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3daa500",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
