{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b46c54",
   "metadata": {
    "id": "b8b46c54"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import Model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models, backend, constraints, initializers\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd9ab35",
   "metadata": {
    "id": "2cd9ab35"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('truncated_LOB_data_BTC_USD_COINBASE.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93636ab5",
   "metadata": {
    "id": "93636ab5"
   },
   "outputs": [],
   "source": [
    "def finder_of_fulfilment(df, column):\n",
    "    time, indicator = [], []\n",
    "    for i in range(len(df)):\n",
    "        num = df[column].iloc[i]\n",
    "        arr = df[df[column]>num][column].index\n",
    "        time.append(pd.to_datetime(df['timestamp'].iloc[arr[arr>i][0]]) - pd.to_datetime(df['timestamp'].iloc[i])\n",
    "                    if len(arr[arr>i]) else\n",
    "                    pd.to_datetime('2023-10-03 00:00:00') - pd.to_datetime(df['timestamp'].iloc[i]))\n",
    "        indicator.append(1 if len(arr[arr>i]) else 0)\n",
    "    return time, indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415cf8e4",
   "metadata": {
    "id": "415cf8e4"
   },
   "outputs": [],
   "source": [
    "df['time'], df['indicator'] = finder_of_fulfilment(df, 'ask_prices_0')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e707e1",
   "metadata": {
    "id": "30e707e1"
   },
   "outputs": [],
   "source": [
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['time'] = (df['time']).dt.total_seconds().astype(int)\n",
    "df = df.iloc[0:-140]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa8749",
   "metadata": {
    "id": "cdfa8749"
   },
   "outputs": [],
   "source": [
    "df_to_plot = df[['indicator', 'time']]\n",
    "N = len(df_to_plot)\n",
    "S_of_t = []\n",
    "\n",
    "for time in range(max(df['time'])):\n",
    "    num_executed = len(df_to_plot[df_to_plot['time']==time])\n",
    "    num_survived = len(df_to_plot[df_to_plot['time']>time])\n",
    "    prob = num_executed/(num_survived+num_executed)\n",
    "    #print(prob)\n",
    "    S_of_t.append(1-prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7036a649",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_hat = np.cumprod(S_of_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedd69cd",
   "metadata": {
    "id": "cedd69cd"
   },
   "outputs": [],
   "source": [
    "t = np.linspace(0, max(df['time']), max(df['time']))\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(t[:400], S_hat[:400], linewidth=2, label='Level 1, pegged', color='black')\n",
    "plt.xlabel('t (sec.)')\n",
    "plt.ylabel('$\\hat{S}(t)$')\n",
    "plt.title('Survival Probability Over Time')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f85fa37",
   "metadata": {
    "id": "1f85fa37"
   },
   "outputs": [],
   "source": [
    "#Make some features and drop useless columns\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "df['vol_imbalance'] = (df['bid_quantity_0'] - df['ask_quantity_0'])/(df['bid_quantity_0'] + df['ask_quantity_0'])\n",
    "df['microprice'] = ((df['bid_prices_0']*df['bid_quantity_0']+df['ask_prices_0']*df['ask_quantity_0'])\n",
    "                    /(df['bid_quantity_0'] + df['ask_quantity_0']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21ff658",
   "metadata": {
    "id": "d21ff658"
   },
   "outputs": [],
   "source": [
    "target = [] #create target S(execution_time)\n",
    "for i in range(len(df)):\n",
    "    target.append(S_of_t[df['time'].iloc[i]-1])\n",
    "\n",
    "df['target'] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be901f2",
   "metadata": {
    "id": "3be901f2"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plotting the microprice with a blue line\n",
    "plt.plot(df['timestamp'], df['microprice'], color='blue', linewidth=2, label='Microprice')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Microprice')\n",
    "plt.title('Microprice over Time')\n",
    "\n",
    "# Displaying the grid\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4916300",
   "metadata": {
    "id": "d4916300"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plotting the microprice with a blue line\n",
    "plt.plot(df['timestamp'], df['vol_imbalance'].rolling(1000).mean(), color='blue',\n",
    "         linewidth=2, label='Rolling Volume Imbalance')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Volume Imbalance')\n",
    "plt.title('Rolling Volume Imbalance over Time')\n",
    "\n",
    "# Displaying the grid\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a396cb78",
   "metadata": {
    "id": "a396cb78"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plotting the microprice with a blue line\n",
    "plt.plot(df['timestamp'], df['target'].rolling(1000).mean(), color='blue',\n",
    "         linewidth=2, label='Target thing')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Target')\n",
    "plt.title('Rolling TargetTargetTarget')\n",
    "\n",
    "# Displaying the grid\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268a1854",
   "metadata": {
    "id": "268a1854"
   },
   "outputs": [],
   "source": [
    "#df.set_index('timestamp', drop=True, inplace=True)\n",
    "scaler = StandardScaler()\n",
    "X = df.drop(columns=['timestamp', 'time', 'indicator', 'target'])\n",
    "Y = df['target'].iloc[500:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7973c179",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_slices = []\n",
    "\n",
    "for i in range(500, len(X)):\n",
    "    # Slice the last 100 rows, including the current row\n",
    "    start_index = max(0, i - 500)  # Adjust index to ensure at least 100 rows\n",
    "    slice_df = X.iloc[start_index:i+1]\n",
    "\n",
    "    # Convert the slice to a tensor and append it to the list\n",
    "    tensor_slice = tf.convert_to_tensor(slice_df, dtype=tf.float32)\n",
    "    tensor_slices.append(tensor_slice)\n",
    "\n",
    "# Stack the slices to create a 3D tensor\n",
    "X = tf.stack(tensor_slices)\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d65bKMHfwMo",
   "metadata": {
    "id": "4d65bKMHfwMo"
   },
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fe5e5e",
   "metadata": {
    "id": "44fe5e5e"
   },
   "source": [
    "# Simplified Staff (LSTM instead of Att)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a2a317",
   "metadata": {
    "id": "ca7949be"
   },
   "source": [
    "input_shape = (100, 22)  # Assuming X is your data\n",
    "steps = 1  # You need to determine the appropriate number of steps based on your data\n",
    "encoder_input = layers.Input(shape=input_shape)\n",
    "encoder_reshape = layers.Reshape(new_input_shape)(encoder_input)\n",
    "encoder_conv1 = layers.Conv1D(32, kernel_size=2, activation='relu')(encoder_reshape)\n",
    "encoder_lstm = layers.LSTM(32, activation='relu')(encoder_conv1)\n",
    "latent_dim = 32\n",
    "encoder_output = layers.Dense(latent_dim)(encoder_lstm)\n",
    "\n",
    "encoder_model = Model(encoder_input, encoder_output)\n",
    "\n",
    "# Define the decoder\n",
    "decoder_input = layers.Input(shape=(latent_dim,))\n",
    "decoder_dense1 = layers.Dense(32, activation='relu')(decoder_input)\n",
    "decoder_reshape = layers.Reshape((1, 32))(decoder_dense1)\n",
    "decoder_conv1 = layers.Conv1D(32, kernel_size=3, activation='relu', padding='same')(decoder_reshape)\n",
    "output_dim = 32\n",
    "decoder_output = layers.Dense(output_dim, activation='sigmoid')(decoder_conv1)\n",
    "\n",
    "decoder_model = Model(decoder_input, decoder_output)\n",
    "\n",
    "# Combine the encoder and decoder into an autoencoder\n",
    "autoencoder_input = layers.Input(shape=input_shape)\n",
    "encoded = encoder_model(autoencoder_input)\n",
    "decoded = decoder_model(encoded)\n",
    "autoencoder_model = Model(autoencoder_input, decoded)\n",
    "\n",
    "# Compile the model\n",
    "autoencoder_model.compile(optimizer='adam', loss='MAE')\n",
    "\n",
    "# Train the model\n",
    "autoencoder_model.fit(X, Y, epochs=3, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294800d1",
   "metadata": {
    "id": "294800d1"
   },
   "source": [
    "# Some experiements (model is almost ready monotonicity change is needed or S(t) to f(t) transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PMu0US83oEG6",
   "metadata": {
    "id": "PMu0US83oEG6"
   },
   "outputs": [],
   "source": [
    "plt.plot(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de282651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DCC layer\n",
    "class DilatedCausalConvolution(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, dilation_rate):\n",
    "        super().__init__()\n",
    "        self.query_conv = layers.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                                        dilation_rate=dilation_rate, padding='same', activation='sigmoid')\n",
    "        self.key_conv = layers.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                                      dilation_rate=dilation_rate, padding='same', activation='sigmoid')\n",
    "        self.value_conv = layers.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                                        dilation_rate=dilation_rate, padding='same', activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query = self.query_conv(inputs)\n",
    "        key = self.key_conv(inputs)\n",
    "        value = self.value_conv(inputs)\n",
    "        return query, key, value\n",
    "\n",
    "# Define the Transformer block\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, num_heads, d_model):\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "\n",
    "    def call(self, query, key, value):\n",
    "        attn_output = self.multi_head_attention(query, key, value)\n",
    "        proj_output = tf.concat(attn_output, axis=0)\n",
    "        return proj_output\n",
    "\n",
    "# Define the encoder using the DCC and Transformer block\n",
    "def create_encoder(input_shape, filters, kernel_size, dilation_rate, num_heads, d_model):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    dcc_layer = DilatedCausalConvolution(filters=filters, kernel_size=kernel_size, dilation_rate=dilation_rate)\n",
    "    query, key, value = dcc_layer(inputs)\n",
    "    transformer_block = TransformerBlock(num_heads=num_heads, d_model=d_model)\n",
    "    transformer_output = transformer_block(query, key, value)\n",
    "    model = models.Model(inputs=inputs, outputs=transformer_output)\n",
    "    return model\n",
    "\n",
    "# Define the monotonic decoder\n",
    "def monotonic_constraint(weight_matrix):\n",
    "    return tf.where(weight_matrix < 0, tf.zeros_like(weight_matrix), weight_matrix)\n",
    "\n",
    "class CustomDense(layers.Layer):\n",
    "    def __init__(self, units, activation='sigmoid', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        last_dim = input_shape[-1] if isinstance(input_shape[-1], int) else input_shape[-1].value\n",
    "        self.kernel = self.add_weight(name='kernel', shape=(last_dim, self.units),\n",
    "                                      initializer='zeros', constraint=monotonic_constraint, trainable=True)\n",
    "        self.bias = self.add_weight(name='bias', shape=(self.units,), initializer='zeros', trainable=False)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.activation(tf.matmul(inputs, self.kernel) + self.bias)\n",
    "\n",
    "class CustomDecoder(layers.Layer):\n",
    "    def __init__(self, input_dim, units, activation='sigmoid', num_layers=5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_layers = num_layers\n",
    "        self.dense_layers = [CustomDense(units, activation=activation) for _ in range(num_layers)]\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.dense_layers[0].build((None, self.input_dim))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.dense_layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# Combine the encoder and decoder to create the full model\n",
    "def create_full_model(input_shape, filters, kernel_size, dilation_rate, num_heads, d_model, output_shape, units):\n",
    "    encoder_inputs = layers.Input(shape=input_shape)\n",
    "    encoder = create_encoder(input_shape, filters, kernel_size, dilation_rate, num_heads, d_model)\n",
    "    encoder_output = encoder(encoder_inputs)\n",
    "\n",
    "    encoder_output_dim = encoder_output.shape[-1]\n",
    "\n",
    "    decoder = CustomDecoder(input_dim=encoder_output_dim, units=units, activation='sigmoid')\n",
    "    decoder_output = decoder(encoder_output)\n",
    "    flat_output = tf.keras.layers.Flatten()(decoder_output)\n",
    "    final_output = tf.keras.layers.Dense(output_shape)(flat_output)\n",
    "    full_model = models.Model(inputs=encoder_inputs, outputs=final_output)\n",
    "\n",
    "    return full_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3daa500",
   "metadata": {
    "id": "c3daa500"
   },
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "input_shape = (501, 22)  # Adjust based on your actual data dimensions\n",
    "filters = 8\n",
    "units = 32\n",
    "kernel_size = 8\n",
    "dilation_rate = 2\n",
    "num_heads = 3\n",
    "d_model = 64\n",
    "output_shape = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdf79bc",
   "metadata": {
    "id": "ebdf79bc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = create_full_model(input_shape, filters, kernel_size, dilation_rate, num_heads, d_model, output_shape, units)\n",
    "\n",
    "# Create TensorFlow dataset\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X, Y)).batch(32)\n",
    "\n",
    "# Compile the model\n",
    "initial_learning_rate = 0.01\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=1000, decay_rate=0.96, staircase=True)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "              loss=tf.keras.losses.MeanAbsoluteError())\n",
    "\n",
    "# Train the model\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=15)\n",
    "model.fit(train_ds, epochs=10, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruqiL6o3YmrK",
   "metadata": {
    "id": "ruqiL6o3YmrK"
   },
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X)\n",
    "\n",
    "Y_pred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DlwoPUF1YugP",
   "metadata": {
    "id": "DlwoPUF1YugP"
   },
   "outputs": [],
   "source": [
    "plt.plot(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biUDVozSdhVw",
   "metadata": {
    "id": "biUDVozSdhVw"
   },
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8v4jo_U_mRmN",
   "metadata": {
    "id": "8v4jo_U_mRmN"
   },
   "outputs": [],
   "source": [
    "model_att.add(SeqSelfAttention(units=1, attention_activation='sigmoid', attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL, input_shape=(X.shape[1], 1)))\n",
    "model_att.add(layers.Flatten())\n",
    "model_att.add(layers.Dense(1, activation='linear'))\n",
    "model_att.compile(loss='mae', optimizer='adam')\n",
    "history = model_att.fit(X, Y, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cvYWmWLToJ-B",
   "metadata": {
    "id": "cvYWmWLToJ-B"
   },
   "outputs": [],
   "source": [
    "SeqSelfAttention."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
