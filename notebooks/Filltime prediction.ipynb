{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b8b46c54",
      "metadata": {
        "id": "b8b46c54"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import optuna\n",
        "import shap\n",
        "from tensorflow.keras.models import Model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers, models, backend, constraints, initializers, regularizers\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install optuna"
      ],
      "metadata": {
        "id": "WCJhhr9do6Cq"
      },
      "id": "WCJhhr9do6Cq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap"
      ],
      "metadata": {
        "id": "FNYEYSV2o-kB"
      },
      "id": "FNYEYSV2o-kB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2cd9ab35",
      "metadata": {
        "id": "2cd9ab35"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('truncated_LOB_data_BTC_USD_COINBASE.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "W7s5kUqFVYEd"
      },
      "id": "W7s5kUqFVYEd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "93636ab5",
      "metadata": {
        "id": "93636ab5"
      },
      "outputs": [],
      "source": [
        "def finder_of_fulfilment(df, column):\n",
        "    time, indicator = [], []\n",
        "    if 'bid' in column:\n",
        "      repr_column = 'ask_prices_0'\n",
        "    else:\n",
        "      repr_column = 'bid_prices_0'\n",
        "    for i in range(len(df)):\n",
        "        num = df[column].iloc[i]\n",
        "        arr = df[df[repr_column]>num][repr_column].index\n",
        "        time.append(pd.to_datetime(df['timestamp'].iloc[arr[arr>i][0]]) - pd.to_datetime(df['timestamp'].iloc[i])\n",
        "                    if len(arr[arr>i]) else\n",
        "                    pd.to_datetime('2023-10-03 00:00:00') - pd.to_datetime(df['timestamp'].iloc[i]))\n",
        "    return time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "columns = ['bid_prices_0', 'ask_prices_0',\t'bid_prices_1',\t'ask_prices_1',\t'bid_prices_2', 'ask_prices_2', 'bid_prices_3', 'ask_prices_3', 'bid_prices_4',\t'ask_prices_4']\n",
        "for column in columns:\n",
        "  df['time_'+column] =  finder_of_fulfilment(df, column)"
      ],
      "metadata": {
        "id": "QqQG4rDMcR07"
      },
      "id": "QqQG4rDMcR07",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for column in columns:\n",
        "  df['time_'+column] = round((df['time_'+column]).dt.total_seconds().astype(float),2)"
      ],
      "metadata": {
        "id": "tyVoIFqYNJp0"
      },
      "id": "tyVoIFqYNJp0",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "IZYI8wRUNnq5"
      },
      "id": "IZYI8wRUNnq5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_KM_estimator(column):\n",
        "  #df['time_'+column] = round((df['time_'+column]).dt.total_seconds().astype(float),2)\n",
        "  times = np.sort(df['time_'+column].unique())\n",
        "  S_of_t = []\n",
        "\n",
        "  for time in times:\n",
        "      num_executed = len(df[df['time_'+column]==time])\n",
        "      num_survived = len(df[df['time_'+column]>time])\n",
        "      prob = num_executed/(num_survived+num_executed)\n",
        "      S_of_t.append(1-prob)\n",
        "\n",
        "\n",
        "  S_hat = np.cumprod(S_of_t)\n",
        "  return S_hat, times"
      ],
      "metadata": {
        "id": "sxzlnwghe7Vf"
      },
      "id": "sxzlnwghe7Vf",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
        "\n",
        "for column in columns:\n",
        "  KM_est, times = plot_KM_estimator(column)\n",
        "  if 'bid' in column:\n",
        "    ax1.plot(times[:400], KM_est[:400], linewidth=2, label=f\"Level {column.split('_')[-1]}, {column.split('_')[0]} pegged\")\n",
        "    ax1.set_xlabel('t (sec.)')\n",
        "    ax1.set_ylabel('$\\hat{S}(t)$')\n",
        "    ax1.set_title('Bid Survival Probability Over Time')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "  else:\n",
        "    ax2.plot(times[:400], KM_est[:400], linewidth=2, label=f\"Level {column.split('_')[-1]}, {column.split('_')[0]} pegged\")\n",
        "    ax2.set_xlabel('t (sec.)')\n",
        "    ax2.set_ylabel('$\\hat{S}(t)$')\n",
        "    ax2.set_title('Ask Survival Probability Over Time')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OLmiTRCLgWKa"
      },
      "id": "OLmiTRCLgWKa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "1f85fa37",
      "metadata": {
        "id": "1f85fa37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70fe093d-2b5c-491c-daa4-9c50502a75db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vol is 0.14% per second\n",
            "Vol is 8.53% per hour\n",
            "Vol is 41.79% per day\n",
            "Vol is 798.71% per year\n"
          ]
        }
      ],
      "source": [
        "#Make some features and drop useless columns\n",
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "df['vol_imbalance'] = (df['bid_quantity_0'] - df['ask_quantity_0'])/(df['bid_quantity_0'] + df['ask_quantity_0'])\n",
        "df['microprice'] = ((df['bid_prices_0']*df['bid_quantity_0']+df['ask_prices_0']*df['ask_quantity_0'])\n",
        "                    /(df['bid_quantity_0'] + df['ask_quantity_0']))\n",
        "\n",
        "def logReturns(S):\n",
        "    return np.log(S/S.shift(1))\n",
        "\n",
        "def volFromTimeSeriesPerSecond(prices, timeStamps):\n",
        "    if np.size(prices) <= 2:\n",
        "        raise ValueError('We need more prices than 2')\n",
        "\n",
        "    # Convert timestamp strings to datetime\n",
        "    timeStamps = pd.to_datetime(timeStamps)\n",
        "\n",
        "    x = logReturns(prices)[1:]  # the value at the first index is NaN\n",
        "    xBar = np.mean(x)\n",
        "\n",
        "    # Calculate deltaT in seconds\n",
        "    deltaT = (timeStamps - timeStamps.shift(10)).dt.total_seconds()[1:]  # the value at the first index is NaN\n",
        "    N = np.size(x)\n",
        "\n",
        "    return np.sqrt(np.sum((x - xBar) ** 2 / deltaT) / (N - 1))\n",
        "\n",
        "def volFromTimeSeriesPerHour(prices,timeStamps):\n",
        "    return volFromTimeSeriesPerSecond(prices, timeStamps)*np.sqrt(3600.0)\n",
        "\n",
        "def volFromTimeSeriesPerDay(prices,timeStamps):\n",
        "    return volFromTimeSeriesPerSecond(prices, timeStamps)*np.sqrt(3600.0*24)\n",
        "\n",
        "def volFromTimeSeriesPerYear(prices,timeStamps):\n",
        "    return volFromTimeSeriesPerSecond(prices, timeStamps)*np.sqrt(3600.0*24*365.25)\n",
        "\n",
        "\n",
        "print(\"Vol is %.2f%% per second\" % (100.0*volFromTimeSeriesPerSecond(df['microprice'], df['timestamp'])))\n",
        "print(\"Vol is %.2f%% per hour\" % (100.0*volFromTimeSeriesPerHour(df['microprice'], df['timestamp'])))\n",
        "print(\"Vol is %.2f%% per day\" % (100.0*volFromTimeSeriesPerDay(df['microprice'], df['timestamp'])))\n",
        "print(\"Vol is %.2f%% per year\" % (100.0*volFromTimeSeriesPerYear(df['microprice'], df['timestamp'])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "wX7XS6pRRO6V",
      "metadata": {
        "id": "wX7XS6pRRO6V"
      },
      "outputs": [],
      "source": [
        "def calculate_volatility(prices, timeStamps):\n",
        "    if len(prices) <= 1:\n",
        "        return np.nan  # Not enough data\n",
        "\n",
        "    x = logReturns(prices)\n",
        "    xBar = np.mean(x)\n",
        "    i=10\n",
        "    deltaT = (timeStamps - timeStamps.shift(i)).dt.total_seconds()\n",
        "    N = len(x)\n",
        "\n",
        "    return np.sqrt(np.nansum((x - xBar) ** 2 / deltaT) / (N - 1))\n",
        "\n",
        "# Define a function to calculate rolling volatility\n",
        "def rolling_volatility(df, window_size):\n",
        "    # Convert timestamp strings to datetime\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "\n",
        "    # Apply the rolling window\n",
        "    rolling_vol = df['microprice'].rolling(window=window_size).apply(\n",
        "        lambda x: calculate_volatility(x, df['timestamp']), raw=False\n",
        "    )\n",
        "\n",
        "    return rolling_vol\n",
        "\n",
        "window_size = 1000\n",
        "\n",
        "# Calculate rolling volatility and add it as a new column\n",
        "df['daily_volatility'] = rolling_volatility(df, window_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3be901f2",
      "metadata": {
        "id": "3be901f2"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "\n",
        "# Plotting the microprice with a blue line\n",
        "plt.plot(df['timestamp'], df['microprice'], color='blue', linewidth=2, label='Microprice')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Microprice')\n",
        "plt.title('Microprice over Time')\n",
        "\n",
        "# Displaying the grid\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4916300",
      "metadata": {
        "id": "d4916300"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plotting the microprice with a blue line\n",
        "plt.plot(df['timestamp'], df['vol_imbalance'].rolling(1000).mean(), color='blue',\n",
        "         linewidth=2, label='Rolling Volume Imbalance')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Volume Imbalance')\n",
        "plt.title('Rolling Volume Imbalance over Time')\n",
        "\n",
        "# Displaying the grid\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W2IOdcadXq3F",
      "metadata": {
        "id": "W2IOdcadXq3F"
      },
      "outputs": [],
      "source": [
        "df['daily_volatility'] = df['daily_volatility']*np.sqrt(360000.0*24)*100\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_B_1CtjkVkVc",
      "metadata": {
        "id": "_B_1CtjkVkVc"
      },
      "outputs": [],
      "source": [
        "df.dropna(inplace=True)\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('prepared_LOB_trunc_data.csv')"
      ],
      "metadata": {
        "id": "8i5niMrpCIJj"
      },
      "id": "8i5niMrpCIJj",
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "268a1854",
      "metadata": {
        "id": "268a1854"
      },
      "outputs": [],
      "source": [
        "N = 100  # Define the number of previous observations included\n",
        "\n",
        "# Assuming 'columns' is a list of column names and 'column' is a specific column name\n",
        "columns_to_drop = ['time_' + col for col in columns] + ['timestamp']\n",
        "X = df.drop(columns=columns_to_drop).iloc[:len(df)//10]\n",
        "Y = df[['time_' + col for col in columns]].iloc[N:len(df)//10].values\n",
        "\n",
        "print(Y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7973c179",
      "metadata": {
        "id": "7973c179"
      },
      "outputs": [],
      "source": [
        "tensor_slices = []\n",
        "\n",
        "for i in range(N, len(X)):\n",
        "    start_index = max(0, i - N)\n",
        "    slice_df = X.iloc[start_index:i+1]\n",
        "\n",
        "    tensor_slice = tf.convert_to_tensor(slice_df, dtype=tf.float32)\n",
        "    tensor_slices.append(tensor_slice)\n",
        "\n",
        "X = tf.stack(tensor_slices)\n",
        "\n",
        "print(X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PMu0US83oEG6",
      "metadata": {
        "id": "PMu0US83oEG6"
      },
      "outputs": [],
      "source": [
        "plt.plot(Y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for column in columns:\n",
        "  df['time_' + column] = df['time_' + column].apply(lambda x: 1 if x < threshold else 0)"
      ],
      "metadata": {
        "id": "h6wAAbeGbtkl"
      },
      "id": "h6wAAbeGbtkl",
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "de282651",
      "metadata": {
        "id": "de282651"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, filters, kernel_size_Q, kernel_size_K, kernel_size_V, units, dilation_rate, num_heads):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.units = units\n",
        "        self.dilation_rate = dilation_rate\n",
        "\n",
        "        # Dilated Convolution layer\n",
        "        self.dilated_conv_Q = tf.keras.layers.Conv1D(kernel_size=kernel_size_Q,\n",
        "                                                  filters=filters,\n",
        "                                                  dilation_rate=self.dilation_rate,\n",
        "                                                  padding='causal',\n",
        "                                                  activation='relu')\n",
        "\n",
        "        self.dilated_conv_K = tf.keras.layers.Conv1D(kernel_size=kernel_size_K,\n",
        "                                                  filters=filters,\n",
        "                                                  dilation_rate=self.dilation_rate,\n",
        "                                                  padding='causal',\n",
        "                                                  activation='relu')\n",
        "\n",
        "        self.dilated_conv_V = tf.keras.layers.Conv1D(kernel_size=kernel_size_V,\n",
        "                                                  filters=filters,\n",
        "                                                  dilation_rate=self.dilation_rate,\n",
        "                                                  padding='causal',\n",
        "                                                  activation='relu')\n",
        "\n",
        "\n",
        "        # Multi-Head Attention layer with 3 heads\n",
        "        self.multi_head_attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=16)\n",
        "\n",
        "        # Flatten layer to reduce to a one-dimensional array\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "\n",
        "    def call(self, x):\n",
        "        # Dilated Convolution\n",
        "        Q = self.dilated_conv_Q(x)\n",
        "        K = self.dilated_conv_K(x)\n",
        "        V = self.dilated_conv_V(x)\n",
        "        # Multi-Head Attention\n",
        "        x = self.multi_head_attention(Q, K, V)\n",
        "\n",
        "        # Flatten the output\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class SimpleDecoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers=10, units_per_layer=64, activation='relu', l1_reg=0.1, dropout_rate=0.1):\n",
        "        super(SimpleDecoder, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.units_per_layer = units_per_layer\n",
        "        self.activation = activation\n",
        "        self.l1_reg = l1_reg\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        # Initialize dense layers with L1 regularization and Dropout layers\n",
        "        self.layers = []\n",
        "        for _ in range(num_layers):\n",
        "            self.layers.append(layers.Dense(units=self.units_per_layer,\n",
        "                                            activation=self.activation,\n",
        "                                            kernel_regularizer=regularizers.l1(self.l1_reg)))\n",
        "            self.layers.append(layers.Dropout(self.dropout_rate))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = inputs\n",
        "        for layer in self.layers:\n",
        "            if isinstance(layer, layers.Dropout):\n",
        "                x = layer(x, training=training)  # Only apply dropout during training\n",
        "            else:\n",
        "                x = layer(x)\n",
        "        return x\n",
        "\n",
        "def create_encoder(input_shape, d_model, num_heads, dilation_rate, kernel_size_Q, kernel_size_K, kernel_size_V):\n",
        "    return Encoder(units=d_model, filters=filters, kernel_size_Q=kernel_size_Q, kernel_size_K=kernel_size_K, kernel_size_V=kernel_size_V, dilation_rate=dilation_rate, num_heads=num_heads)\n",
        "\n",
        "# Combine the encoder and decoder to create the full model\n",
        "def create_full_model(input_shape, filters, num_heads, d_model, output_shape, units, kernel_size_Q, kernel_size_K, kernel_size_V):\n",
        "    encoder_inputs = layers.Input(shape=input_shape)\n",
        "    encoder = create_encoder(input_shape, d_model, num_heads, filters, kernel_size_Q, kernel_size_K, kernel_size_V)\n",
        "    encoder_output = encoder(encoder_inputs)\n",
        "\n",
        "    decoder = SimpleDecoder()\n",
        "    decoder_output = decoder(encoder_output)\n",
        "\n",
        "    # Flatten the decoder output if not already flattened\n",
        "    flat_output = tf.keras.layers.Flatten()(decoder_output)\n",
        "    output = tf.keras.layers.Dense(output_shape, activation= 'sigmoid')(flat_output)\n",
        "    full_model = tf.keras.models.Model(inputs=encoder_inputs, outputs=output)\n",
        "\n",
        "    return full_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdf8a504",
      "metadata": {
        "scrolled": true,
        "id": "bdf8a504",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "705c9ca8-2e01-456b-ce4b-469c82e2dbba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "632/632 [==============================] - 18s 28ms/step - loss: 68.4302\n",
            "Epoch 52/100\n",
            "435/632 [===================>..........] - ETA: 4s - loss: 68.2253"
          ]
        }
      ],
      "source": [
        "# Define model parameters\n",
        "input_shape = (N+1, 23)\n",
        "filters = 2\n",
        "units = 4\n",
        "kernel_size_Q, kernel_size_K, kernel_size_V = 2, 3, 4\n",
        "num_heads = 3\n",
        "d_model = 2\n",
        "output_shape = Y.shape[1]\n",
        "batch = 16\n",
        "\n",
        "# Create the model\n",
        "model = create_full_model(input_shape, filters, num_heads, d_model, output_shape, units, kernel_size_Q, kernel_size_K, kernel_size_V)\n",
        "model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=10**(-4)),\n",
        "              loss='binary_crossentropy')\n",
        "\n",
        "\n",
        "# Train the model\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
        "model.fit(x=X, y=Y, batch_size=batch, epochs=100, callbacks=[callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81529074",
      "metadata": {
        "id": "81529074"
      },
      "outputs": [],
      "source": [
        "Y_pred = model.predict(X)\n",
        "\n",
        "plt.plot(Y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "p4P8ht8jyePD",
      "metadata": {
        "id": "p4P8ht8jyePD"
      },
      "outputs": [],
      "source": [
        "# Define model parameters\n",
        "input_shape = (N+1, 23)\n",
        "output_shape = Y.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "biUDVozSdhVw",
      "metadata": {
        "id": "biUDVozSdhVw",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Optuna objective function\n",
        "def objective(trial):\n",
        "    # Suggest values for the hyperparameters\n",
        "    num_heads = trial.suggest_categorical('num_heads', [2, 4, 8, 16])\n",
        "    filters = trial.suggest_categorical('filters', [2, 4, 8, 16])\n",
        "    units = trial.suggest_categorical('units', [1, 4, 8, 16, 32])\n",
        "    kernel_size_Q = trial.suggest_categorical('kernel_size_Q', [2, 4, 8, 16])\n",
        "    kernel_size_K = trial.suggest_categorical('kernel_size_K', [2, 4, 8, 16])\n",
        "    kernel_size_V = trial.suggest_categorical('kernel_size_V', [2, 4, 8, 16])\n",
        "    dilation_rate = trial.suggest_categorical('dilation_rate', [1, 2, 3, 4])\n",
        "    d_model = trial.suggest_categorical('d_model', [1, 2, 4, 8, 32])\n",
        "    batch = trial.suggest_categorical('batch', [4, 8, 16, 32])\n",
        "    learning_rate = trial.suggest_categorical('learning_rate', [1e-6, 1e-5, 1e-4, 1e-3, 1e-1])  # Suggest learning rate\n",
        "\n",
        "    # Create the model using the suggested values\n",
        "    model = create_full_model(input_shape, filters, kernel_size, num_heads, d_model, output_shape, units)\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate),loss='mse')\n",
        "\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((X, Y)).batch(batch_size = batch)\n",
        "\n",
        "    callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
        "    history = model.fit(train_ds, epochs=50, callbacks=[callback], verbose=1)\n",
        "\n",
        "    val_loss = sum(history.history['loss'][-6:-1])/5\n",
        "    return val_loss\n",
        "\n",
        "# Optuna study\n",
        "study = optuna.create_study(direction='minimize')  # 'minimize' if lower loss is better, 'maximize' otherwise\n",
        "study.optimize(objective, n_trials=50)  # Adjust the number of trials\n",
        "\n",
        "# Print the optimal hyperparameters\n",
        "print('Best trial:')\n",
        "trial = study.best_trial\n",
        "print(f'  Value: {trial.value}')\n",
        "print('  Params: ')\n",
        "for key, value in trial.params.items():\n",
        "    print(f'    {key}: {value}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wgPEvxifbRzO",
      "metadata": {
        "id": "wgPEvxifbRzO",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Define model parameters\n",
        "input_shape = (N+1, 23)\n",
        "output_shape = Y.shape[1]\n",
        "num_heads = trial.params['num_heads']\n",
        "filters = trial.params['filters']\n",
        "units = trial.params['units']\n",
        "kernel_size = trial.params['kernel_size']\n",
        "d_model = trial.params['d_model']\n",
        "batch = trial.params['batch']\n",
        "learning_rate = trial.params['learning_rate']\n",
        "# Create the model\n",
        "model = create_full_model(input_shape, filters, kernel_size, num_heads, d_model, output_shape, units)\n",
        "\n",
        "# Create TensorFlow dataset\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X, Y)).batch(batch_size=batch)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate),\n",
        "              loss='mse')\n",
        "\n",
        "# Train the model\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=15)\n",
        "model.fit(train_ds, epochs=100, callbacks=[callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Fo-pZrxIdoAa",
      "metadata": {
        "id": "Fo-pZrxIdoAa"
      },
      "outputs": [],
      "source": [
        "Y_pred = model.predict(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "r-WXXEZlhRlI",
      "metadata": {
        "id": "r-WXXEZlhRlI"
      },
      "outputs": [],
      "source": [
        "plt.plot(Y_pred)\n",
        "plt.plot(Y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "explainer = shap.TreeExplainer(model)\n",
        "shap_values = explainer.shap_values(X)\n",
        "shap.summary_plot(shap_values, X)"
      ],
      "metadata": {
        "id": "RdPaIZUnzho9"
      },
      "id": "RdPaIZUnzho9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap_explanation = shap.Explanation(\n",
        "    values=shap_values,\n",
        "    base_values=explainer.expected_value,\n",
        "    data=X,  # or a subset of X for large datasets\n",
        "    feature_names=X.columns,  # replace with your actual feature names\n",
        "    output_names=[\"Prediction\"]\n",
        ")\n",
        "\n",
        "# Generate the waterfall plot for the first prediction\n",
        "shap.waterfall_plot(shap_explanation[0])"
      ],
      "metadata": {
        "id": "fej8wwvNp1a5"
      },
      "id": "fej8wwvNp1a5",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}