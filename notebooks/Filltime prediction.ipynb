{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8b46c54",
      "metadata": {
        "id": "b8b46c54"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.models import Model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers, models, backend, constraints, initializers\n",
        "from keras_self_attention import SeqSelfAttention\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cd9ab35",
      "metadata": {
        "id": "2cd9ab35"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('truncated_LOB_data_BTC_USD_COINBASE.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93636ab5",
      "metadata": {
        "id": "93636ab5"
      },
      "outputs": [],
      "source": [
        "def finder_of_fulfilment(df, column):\n",
        "    time, indicator = [], []\n",
        "    for i in range(len(df)):\n",
        "        num = df[column].iloc[i]\n",
        "        arr = df[df[column]>num][column].index\n",
        "        time.append(pd.to_datetime(df['timestamp'].iloc[arr[arr>i][0]]) - pd.to_datetime(df['timestamp'].iloc[i])\n",
        "                    if len(arr[arr>i]) else\n",
        "                    pd.to_datetime('2023-10-03 00:00:00') - pd.to_datetime(df['timestamp'].iloc[i]))\n",
        "        indicator.append(1 if len(arr[arr>i]) else 0)\n",
        "    return time, indicator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "415cf8e4",
      "metadata": {
        "id": "415cf8e4"
      },
      "outputs": [],
      "source": [
        "df['time'], df['indicator'] = finder_of_fulfilment(df, 'ask_prices_0')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30e707e1",
      "metadata": {
        "id": "30e707e1"
      },
      "outputs": [],
      "source": [
        "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "df['time'] = (df['time']).dt.total_seconds().astype(int)\n",
        "df = df.iloc[0:-140]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdfa8749",
      "metadata": {
        "id": "cdfa8749"
      },
      "outputs": [],
      "source": [
        "df_to_plot = df[['indicator', 'time']]\n",
        "N = len(df_to_plot)\n",
        "F_of_t = []\n",
        "\n",
        "for time in range(max(df['time'])):\n",
        "    num_executed = len(df_to_plot[df_to_plot['time']<time])\n",
        "    prob = num_executed/N\n",
        "    F_of_t.append(prob)\n",
        "\n",
        "S_of_t = 1 - np.array(F_of_t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cedd69cd",
      "metadata": {
        "id": "cedd69cd"
      },
      "outputs": [],
      "source": [
        "t = np.linspace(0, max(df['time']), max(df['time']))\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(t[:400], S_of_t[:400], linewidth=2, label='Level 1, pegged', color='black')\n",
        "plt.xlabel('t (sec.)')\n",
        "plt.ylabel('$\\hat{S}(t)$')\n",
        "plt.title('Survival Probability Over Time')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f85fa37",
      "metadata": {
        "id": "1f85fa37"
      },
      "outputs": [],
      "source": [
        "#Make some features and drop useless columns\n",
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "df['vol_imbalance'] = (df['bid_quantity_0'] - df['ask_quantity_0'])/(df['bid_quantity_0'] + df['ask_quantity_0'])\n",
        "df['microprice'] = ((df['bid_prices_0']*df['bid_quantity_0']+df['ask_prices_0']*df['ask_quantity_0'])\n",
        "                    /(df['bid_quantity_0'] + df['ask_quantity_0']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d21ff658",
      "metadata": {
        "id": "d21ff658"
      },
      "outputs": [],
      "source": [
        "target = [] #create target S(execution_time)\n",
        "for i in range(len(df)):\n",
        "    target.append(S_of_t[df['time'].iloc[i]-1])\n",
        "\n",
        "df['target'] = target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3be901f2",
      "metadata": {
        "id": "3be901f2"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plotting the microprice with a blue line\n",
        "plt.plot(df['timestamp'], df['microprice'], color='blue', linewidth=2, label='Microprice')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Microprice')\n",
        "plt.title('Microprice over Time')\n",
        "\n",
        "# Displaying the grid\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4916300",
      "metadata": {
        "id": "d4916300"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plotting the microprice with a blue line\n",
        "plt.plot(df['timestamp'], df['vol_imbalance'].rolling(1000).mean(), color='blue',\n",
        "         linewidth=2, label='Rolling Volume Imbalance')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Volume Imbalance')\n",
        "plt.title('Rolling Volume Imbalance over Time')\n",
        "\n",
        "# Displaying the grid\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a396cb78",
      "metadata": {
        "id": "a396cb78"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plotting the microprice with a blue line\n",
        "plt.plot(df['timestamp'], df['target'].rolling(1000).mean(), color='blue',\n",
        "         linewidth=2, label='Target thing')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Target')\n",
        "plt.title('Rolling TargetTargetTarget')\n",
        "\n",
        "# Displaying the grid\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "268a1854",
      "metadata": {
        "id": "268a1854"
      },
      "outputs": [],
      "source": [
        "#df.set_index('timestamp', drop=True, inplace=True)\n",
        "scaler = StandardScaler()\n",
        "X = df.drop(columns=['time', 'indicator', 'target'])\n",
        "Y = df['target'].values"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y = scaler.fit_transform(Y.reshape(-1, 1))"
      ],
      "metadata": {
        "id": "4d65bKMHfwMo"
      },
      "id": "4d65bKMHfwMo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "44fe5e5e",
      "metadata": {
        "id": "44fe5e5e"
      },
      "source": [
        "# Simplified Staff (LSTM instead of Att)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca7949be",
      "metadata": {
        "id": "ca7949be"
      },
      "outputs": [],
      "source": [
        "input_shape = X.shape[1:]  # Assuming X is your data\n",
        "steps = 1  # You need to determine the appropriate number of steps based on your data\n",
        "new_input_shape = (input_shape[0], steps)\n",
        "encoder_input = layers.Input(shape=input_shape)\n",
        "encoder_reshape = layers.Reshape(new_input_shape)(encoder_input)\n",
        "encoder_conv1 = layers.Conv1D(32, kernel_size=2, activation='relu')(encoder_reshape)\n",
        "encoder_lstm = layers.LSTM(32, activation='relu')(encoder_conv1)\n",
        "latent_dim = 32\n",
        "encoder_output = layers.Dense(latent_dim)(encoder_lstm)\n",
        "\n",
        "encoder_model = Model(encoder_input, encoder_output)\n",
        "\n",
        "# Define the decoder\n",
        "decoder_input = layers.Input(shape=(latent_dim,))\n",
        "decoder_dense1 = layers.Dense(32, activation='relu')(decoder_input)\n",
        "decoder_reshape = layers.Reshape((1, 32))(decoder_dense1)\n",
        "decoder_conv1 = layers.Conv1D(32, kernel_size=3, activation='relu', padding='same')(decoder_reshape)\n",
        "output_dim = 32\n",
        "decoder_output = layers.Dense(output_dim, activation='sigmoid')(decoder_conv1)\n",
        "\n",
        "decoder_model = Model(decoder_input, decoder_output)\n",
        "\n",
        "# Combine the encoder and decoder into an autoencoder\n",
        "autoencoder_input = layers.Input(shape=input_shape)\n",
        "encoded = encoder_model(autoencoder_input)\n",
        "decoded = decoder_model(encoded)\n",
        "autoencoder_model = Model(autoencoder_input, decoded)\n",
        "\n",
        "# Compile the model\n",
        "autoencoder_model.compile(optimizer='adam', loss='MAE')\n",
        "\n",
        "# Train the model\n",
        "autoencoder_model.fit(X, Y, epochs=3, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "294800d1",
      "metadata": {
        "id": "294800d1"
      },
      "source": [
        "# Some experiements (model is almost ready monotonicity change is needed or S(t) to f(t) transition)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf914664",
      "metadata": {
        "id": "bf914664"
      },
      "outputs": [],
      "source": [
        "# Define the DCC layer (updated version)\n",
        "class DilatedCausalConvolution(layers.Layer):\n",
        "    def __init__(self, filters, kernel_size, dilation_rate):\n",
        "        super().__init__()\n",
        "        self.query_conv = layers.Conv1D(filters=filters, kernel_size=kernel_size,\n",
        "                                        dilation_rate=dilation_rate, padding='same', activation='sigmoid')\n",
        "        self.key_conv = layers.Conv1D(filters=filters, kernel_size=kernel_size,\n",
        "                                      dilation_rate=dilation_rate, padding='same', activation='sigmoid')\n",
        "        self.value_conv = layers.Conv1D(filters=filters, kernel_size=kernel_size,\n",
        "                                        dilation_rate=dilation_rate, padding='same', activation='sigmoid')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        query = self.query_conv(inputs)\n",
        "        key = self.key_conv(inputs)\n",
        "        value = self.value_conv(inputs)\n",
        "        return query, key, value\n",
        "\n",
        "# Define the Transformer block (updated version)\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, num_heads, d_model):\n",
        "        super().__init__()\n",
        "        self.multi_head_attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "    def call(self, query, key, value):\n",
        "        attn_output = self.multi_head_attention(query, key, value)\n",
        "        proj_output = tf.concat(attn_output, axis=0)\n",
        "        return proj_output\n",
        "\n",
        "# Define the encoder using the DCC and Transformer block (updated version)\n",
        "def create_encoder(input_shape, filters, kernel_size, dilation_rate, num_heads, d_model):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    dcc_layer = DilatedCausalConvolution(filters=filters, kernel_size=kernel_size, dilation_rate=dilation_rate)\n",
        "    query, key, value = dcc_layer(inputs)\n",
        "    transformer_block = TransformerBlock(num_heads=num_heads, d_model=d_model)\n",
        "    transformer_output = transformer_block(query, key, value)\n",
        "    model = models.Model(inputs=inputs, outputs=transformer_output)\n",
        "    #print(model.weights)\n",
        "    return model\n",
        "\n",
        "# Define the monotonic decoder (updated version)\n",
        "def monotonic_constraint(weight_matrix):\n",
        "    # Constraint for monotonicity: weights must be non-negative\n",
        "    return tf.where(weight_matrix < 0, tf.zeros_like(weight_matrix), weight_matrix)\n",
        "\n",
        "class CustomDense(layers.Layer):\n",
        "    def __init__(self, units, activation='sigmoid', **kwargs):\n",
        "        super(CustomDense, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.activation = tf.keras.activations.get(activation)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Ensure the last dimension of input_shape is an integer\n",
        "        last_dim = input_shape[-1] if isinstance(input_shape[-1], int) else input_shape[-1].value\n",
        "\n",
        "        # Constraint for the weights to be non-negative for monotonicity\n",
        "        self.kernel = self.add_weight(\n",
        "            name='kernel',\n",
        "            shape=(last_dim, self.units),\n",
        "            initializer='zeros',\n",
        "            constraint=monotonic_constraint,\n",
        "            trainable=True\n",
        "        )\n",
        "        self.bias = self.add_weight(\n",
        "            name='bias',\n",
        "            shape=(self.units,),\n",
        "            initializer='zeros',\n",
        "            trainable=False\n",
        "        )\n",
        "        super(CustomDense, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Apply monotonic constraint during the forward pass\n",
        "        return self.activation(tf.matmul(inputs, self.kernel) + self.bias)\n",
        "\n",
        "\n",
        "class CustomDecoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_dim, units, activation='sigmoid', num_layers=5, **kwargs):\n",
        "        super(CustomDecoder, self).__init__(**kwargs)\n",
        "        self.num_layers = num_layers\n",
        "        self.dense_layers = [CustomDense(units, activation=activation) for _ in range(num_layers)]\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # First layer needs to know input shape\n",
        "        self.dense_layers[0].build((None, self.input_dim))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = inputs\n",
        "        for layer in self.dense_layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "# Combine the encoder and decoder to create the full model (updated version)\n",
        "def create_full_model(input_shape, filters, kernel_size, dilation_rate, num_heads, d_model, output_shape):\n",
        "    encoder_inputs = layers.Input(shape=input_shape)\n",
        "    encoder = create_encoder(input_shape, filters, kernel_size, dilation_rate, num_heads, d_model)\n",
        "    encoder_output = encoder(encoder_inputs)\n",
        "\n",
        "    encoder_output_dim = encoder_output.shape[-1]  # This should be an integer\n",
        "\n",
        "    decoder = CustomDecoder(input_dim=encoder_output_dim, units=units, activation='sigmoid')\n",
        "    decoder_output = decoder(encoder_output)\n",
        "    flat_output = tf.keras.layers.Flatten()(decoder_output)\n",
        "    final_output = tf.keras.layers.Dense(1)(flat_output)\n",
        "    full_model = models.Model(inputs=encoder_inputs, outputs=final_output)\n",
        "\n",
        "    return full_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(Y)"
      ],
      "metadata": {
        "id": "PMu0US83oEG6"
      },
      "execution_count": null,
      "outputs": [],
      "id": "PMu0US83oEG6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e114013f",
      "metadata": {
        "id": "e114013f"
      },
      "outputs": [],
      "source": [
        "X_expanded = np.expand_dims(X, axis=1)  # Shape becomes (99601, 22, 1)\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_expanded, Y)).batch(32)\n",
        "\n",
        "# Define model parameters\n",
        "input_shape = (1, 22)  # 22 features per 1 timestamp\n",
        "filters = 64\n",
        "units = 32\n",
        "kernel_size = 32\n",
        "dilation_rate = 3\n",
        "num_heads = 3\n",
        "d_model = 64\n",
        "output_shape = X_expanded.shape[0]\n",
        "\n",
        "initial_learning_rate = 0.01\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate,\n",
        "    decay_steps=1000,\n",
        "    decay_rate=0.96,\n",
        "    staircase=True)\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
        "\n",
        "model = create_full_model(input_shape, filters, kernel_size, dilation_rate, num_heads, d_model, output_shape)\n",
        "model.compile()\n",
        "loss_object = tf.keras.losses.MeanAbsoluteError()\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3daa500",
      "metadata": {
        "id": "c3daa500"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(X, y):\n",
        "  with tf.GradientTape() as tape:\n",
        "    # training=True is only needed if there are layers with different\n",
        "    # behavior during training versus inference (e.g. Dropout).\n",
        "    predictions = model(X, training=True)\n",
        "    loss = loss_object(y, predictions)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "  train_loss(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebdf79bc",
      "metadata": {
        "id": "ebdf79bc"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10\n",
        "for epoch in range(EPOCHS):\n",
        "  # Reset the metrics at the start of the next epoch\n",
        "  train_loss.reset_states()\n",
        "\n",
        "  for features, s in train_ds:\n",
        "    train_step(features, s)\n",
        "\n",
        "  print(\n",
        "    f'Epoch {epoch + 1}, '\n",
        "    f'Loss: {train_loss.result()}, '\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred = model.predict(X_expanded)\n",
        "\n",
        "Y_pred.mean()"
      ],
      "metadata": {
        "id": "ruqiL6o3YmrK"
      },
      "id": "ruqiL6o3YmrK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(Y_pred)"
      ],
      "metadata": {
        "id": "DlwoPUF1YugP"
      },
      "id": "DlwoPUF1YugP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(df['target'])"
      ],
      "metadata": {
        "id": "biUDVozSdhVw"
      },
      "id": "biUDVozSdhVw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_att.add(SeqSelfAttention(units=1, attention_activation='sigmoid', attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL, input_shape=(X.shape[1], 1)))\n",
        "model_att.add(layers.Flatten())\n",
        "model_att.add(layers.Dense(1, activation='linear'))\n",
        "model_att.compile(loss='mae', optimizer='adam')\n",
        "history = model_att.fit(X, Y, epochs=100, verbose=2)"
      ],
      "metadata": {
        "id": "8v4jo_U_mRmN"
      },
      "id": "8v4jo_U_mRmN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SeqSelfAttention."
      ],
      "metadata": {
        "id": "cvYWmWLToJ-B"
      },
      "id": "cvYWmWLToJ-B",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}