{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b46c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import Model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models, backend, constraints, initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd9ab35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('truncated_LOB_data_BTC_USD_COINBASE.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93636ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finder_of_fulfilment(df, column):\n",
    "    time, indicator = [], []\n",
    "    for i in range(len(df)):\n",
    "        num = df[column].iloc[i]\n",
    "        arr = df[df[column]>num][column].index\n",
    "        time.append(pd.to_datetime(df['timestamp'].iloc[arr[arr>i][0]]) - pd.to_datetime(df['timestamp'].iloc[i]) \n",
    "                    if len(arr[arr>i]) else \n",
    "                    pd.to_datetime('2023-10-03 00:00:00') - pd.to_datetime(df['timestamp'].iloc[i]))\n",
    "        indicator.append(1 if len(arr[arr>i]) else 0)\n",
    "    return time, indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415cf8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time'], df['indicator'] = finder_of_fulfilment(df, 'ask_prices_0')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e707e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['time'] = (df['time']).dt.total_seconds().astype(int)\n",
    "df = df.iloc[0:-140]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb9eeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_plot = df[['indicator', 'time']]\n",
    "N = len(df_to_plot)\n",
    "F_of_t = []\n",
    "\n",
    "for time in range(max(df['time'])):\n",
    "    num_executed = len(df_to_plot[df_to_plot['time']<time])\n",
    "    prob = num_executed/N\n",
    "    F_of_t.append(prob)\n",
    "\n",
    "S_of_t = 1 - np.array(F_of_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f234980e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(0, max(df['time']), max(df['time']))\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(t[:400], S_of_t[:400], linewidth=2, label='Level 1, pegged', color='black')\n",
    "plt.xlabel('t (sec.)')\n",
    "plt.ylabel('$\\hat{S}(t)$')\n",
    "plt.title('Survival Probability Over Time')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f85fa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make some features and drop useless columns \n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "df['vol_imbalance'] = (df['bid_quantity_0'] - df['ask_quantity_0'])/(df['bid_quantity_0'] + df['ask_quantity_0'])\n",
    "df['microprice'] = ((df['bid_prices_0']*df['bid_quantity_0']+df['ask_prices_0']*df['ask_quantity_0'])\n",
    "                    /(df['bid_quantity_0'] + df['ask_quantity_0']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185b9975",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [] #create target S(execution_time)\n",
    "for i in range(len(df)):\n",
    "    target.append(S_of_t[df['time'].iloc[i]-1])\n",
    "    \n",
    "df['target'] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be901f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plotting the microprice with a blue line\n",
    "plt.plot(df['timestamp'], df['microprice'], color='blue', linewidth=2, label='Microprice')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Microprice')\n",
    "plt.title('Microprice over Time')\n",
    "\n",
    "# Displaying the grid\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ce698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plotting the microprice with a blue line\n",
    "plt.plot(df['timestamp'], df['vol_imbalance'].rolling(1000).mean(), color='blue',\n",
    "         linewidth=2, label='Rolling Volume Imbalance')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Volume Imbalance')\n",
    "plt.title('Rolling Volume Imbalance over Time')\n",
    "\n",
    "# Displaying the grid\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fa7c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plotting the microprice with a blue line\n",
    "plt.plot(df['timestamp'], df['target'].rolling(1000).mean(), color='blue',\n",
    "         linewidth=2, label='Target thing')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Target')\n",
    "plt.title('Rolling TargetTargetTarget')\n",
    "\n",
    "# Displaying the grid\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268a1854",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('timestamp', drop=True, inplace=True)\n",
    "\n",
    "X = df.drop(columns=['time', 'indicator', 'target'])\n",
    "Y = df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fe5e5e",
   "metadata": {},
   "source": [
    "# Simplified Staff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7949be",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X.shape[1:]  # Assuming X is your data\n",
    "steps = 1  # You need to determine the appropriate number of steps based on your data\n",
    "new_input_shape = (input_shape[0], steps)\n",
    "encoder_input = Input(shape=input_shape)\n",
    "encoder_reshape = Reshape(new_input_shape)(encoder_input)\n",
    "encoder_conv1 = Conv1D(32, kernel_size=2, activation='relu')(encoder_reshape)\n",
    "encoder_lstm = LSTM(32, activation='relu')(encoder_conv1)\n",
    "latent_dim = 32\n",
    "encoder_output = Dense(latent_dim)(encoder_lstm)\n",
    "\n",
    "encoder_model = Model(encoder_input, encoder_output)\n",
    "\n",
    "# Define the decoder\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "decoder_dense1 = Dense(32, activation='relu')(decoder_input)\n",
    "decoder_reshape = Reshape((1, 32))(decoder_dense1)\n",
    "decoder_conv1 = Conv1D(32, kernel_size=3, activation='relu', padding='same')(decoder_reshape)\n",
    "output_dim = 32\n",
    "decoder_output = Dense(output_dim, activation='sigmoid')(decoder_conv1)\n",
    "\n",
    "decoder_model = Model(decoder_input, decoder_output)\n",
    "\n",
    "# Combine the encoder and decoder into an autoencoder\n",
    "autoencoder_input = Input(shape=input_shape)\n",
    "encoded = encoder_model(autoencoder_input)\n",
    "decoded = decoder_model(encoded)\n",
    "autoencoder_model = Model(autoencoder_input, decoded)\n",
    "\n",
    "# Compile the model\n",
    "autoencoder_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "autoencoder_model.fit(X, Y, epochs=3, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294800d1",
   "metadata": {},
   "source": [
    "# some experiements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf914664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DCC layer (updated version)\n",
    "class DilatedCausalConvolution(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, dilation_rate):\n",
    "        super().__init__()\n",
    "        self.query_conv = layers.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                                        dilation_rate=dilation_rate, padding='causal', activation='relu')\n",
    "        self.key_conv = layers.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                                      dilation_rate=dilation_rate, padding='causal', activation='relu')\n",
    "        self.value_conv = layers.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                                        dilation_rate=dilation_rate, padding='causal', activation='relu')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query = self.query_conv(inputs)\n",
    "        key = self.key_conv(inputs)\n",
    "        value = self.value_conv(inputs)\n",
    "        return query, key, value\n",
    "\n",
    "# Define the Transformer block (updated version)\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, num_heads, d_model):\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.dense_proj = layers.Dense(d_model, activation='relu')\n",
    "\n",
    "    def call(self, query, key, value):\n",
    "        attn_output = self.multi_head_attention(query, key, value)\n",
    "        proj_output = self.dense_proj(attn_output)\n",
    "        return proj_output\n",
    "\n",
    "# Define the encoder using the DCC and Transformer block (updated version)\n",
    "def create_encoder(input_shape, filters, kernel_size, dilation_rate, num_heads, d_model):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    dcc_layer = DilatedCausalConvolution(filters=filters, kernel_size=kernel_size, dilation_rate=dilation_rate)\n",
    "    query, key, value = dcc_layer(inputs)\n",
    "    transformer_block = TransformerBlock(num_heads=num_heads, d_model=d_model)\n",
    "    transformer_output = transformer_block(query, key, value)\n",
    "    model = models.Model(inputs=inputs, outputs=transformer_output)\n",
    "    return model\n",
    "\n",
    "# Define the monotonic decoder (updated version)\n",
    "def monotonic_constraint(weight_matrix):\n",
    "    # Constraint for monotonicity: weights must be non-negative\n",
    "    return tf.where(weight_matrix < 0., tf.zeros_like(weight_matrix), weight_matrix)\n",
    "\n",
    "class CustomDense(layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super(CustomDense, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Constraint for the weights to be non-negative for monotonicity\n",
    "        self.kernel = self.add_weight(name='kernel', \n",
    "                                      shape=(input_shape[-1], self.units),\n",
    "                                      initializer=initializers.GlorotUniform(),\n",
    "                                      constraint=monotonic_constraint,\n",
    "                                      trainable=True)\n",
    "        self.bias = self.add_weight(name='bias', \n",
    "                                    shape=(self.units,),\n",
    "                                    initializer='zeros',\n",
    "                                    trainable=True)\n",
    "        super(CustomDense, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Apply monotonic constraint during the forward pass\n",
    "        return self.activation(tf.matmul(inputs, self.kernel) + self.bias)\n",
    "\n",
    "def create_monotonic_decoder(input_shape, output_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Custom dense layer with non-negative weights and sigmoid activation to ensure output is between 0 and 1\n",
    "    outputs = CustomDense(output_shape, activation='sigmoid')(inputs)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Combine the encoder and decoder to create the full model (updated version)\n",
    "def create_full_model(input_shape, filters, kernel_size, dilation_rate, num_heads, d_model, output_shape):\n",
    "    encoder_inputs = layers.Input(shape=input_shape)\n",
    "    encoder = create_encoder(input_shape, filters, kernel_size, dilation_rate, num_heads, d_model)\n",
    "    encoder_output = encoder(encoder_inputs)\n",
    "    \n",
    "    flattened_output = layers.Flatten()(encoder_output)\n",
    "    \n",
    "    decoder = create_monotonic_decoder(flattened_output.shape[1:], output_shape)\n",
    "    decoder_output = decoder(flattened_output)\n",
    "    \n",
    "    full_model = models.Model(inputs=encoder_inputs, outputs=decoder_output)\n",
    "    return full_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e114013f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand dimensions of X to add the channel dimension if needed\n",
    "X_expanded = np.expand_dims(X, axis=-1)  # Shape becomes (99601, 22, 1)\n",
    "\n",
    "# Define model parameters\n",
    "input_shape = (22, 1)  # 22 timesteps and 1 feature per timestep?\n",
    "filters = 64\n",
    "kernel_size = 3\n",
    "dilation_rate = 1\n",
    "num_heads = 8\n",
    "d_model = 64\n",
    "output_shape = 1\n",
    "\n",
    "model = create_full_model(input_shape, filters, kernel_size, dilation_rate, num_heads, d_model, output_shape)\n",
    "model.compile(optimizer='adam', loss='MSE')\n",
    "model.fit(X_expanded, Y, epochs=3, batch_size=32)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3daa500",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_expanded)\n",
    "Y_pred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ced7d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
