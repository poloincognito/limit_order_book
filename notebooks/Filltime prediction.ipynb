{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8b46c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Input, Conv1D, LSTM, Dense, Reshape, Concatenate, Attention, Reshape, Input, Dropout, MultiHeadAttention\n",
    "from keras.models import Model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cd9ab35",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/limit_order_book/truncated_LOB_data_BTC_USD_COINBASE.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/limit_order_book/truncated_LOB_data_BTC_USD_COINBASE.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/conda/miniconda3/envs/myenv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/Desktop/conda/miniconda3/envs/myenv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Desktop/conda/miniconda3/envs/myenv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/Desktop/conda/miniconda3/envs/myenv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1706\u001b[0m     f,\n\u001b[1;32m   1707\u001b[0m     mode,\n\u001b[1;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1714\u001b[0m )\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Desktop/conda/miniconda3/envs/myenv/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/limit_order_book/truncated_LOB_data_BTC_USD_COINBASE.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93636ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finder_of_fulfilment(df, column):\n",
    "    counter = []\n",
    "    for i in range(len(df)):\n",
    "        num = df[column].iloc[i]\n",
    "        arr = df[df[column]>num][column].index\n",
    "        counter.append(df['timestamp'].iloc[arr[arr>i][0]] if len(arr[arr>i]) else -1)\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415cf8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['counter'] = finder_of_fulfilment(df, 'ask_prices_0')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e707e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['counter'] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ecd499",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['counter'] = pd.to_datetime(df['counter'])\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['exec_time'] = (df['counter'] - df['timestamp']).dt.total_seconds().astype(int)\n",
    "df = df.iloc[0:-140]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f85fa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make some features and drop useless columns \n",
    "df.drop(columns = ['timestamp', 'counter'], inplace=True)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "df['vol_imbalance'] = (df['bid_quantity_0'] - df['ask_quantity_0'])/(df['bid_quantity_0'] + df['ask_quantity_0'])\n",
    "df['microprice'] = ((df['bid_prices_0']*df['bid_quantity_0']+df['ask_prices_0']*df['ask_quantity_0'])\n",
    "                    /(df['bid_quantity_0'] + df['ask_quantity_0']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be901f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(10,6))\n",
    "plt.plot(df['microprice'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268a1854",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['exec_time'])\n",
    "Y = df['exec_time']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fe5e5e",
   "metadata": {},
   "source": [
    "# Simplified Staff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7949be",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X.shape[1:]  # Assuming X is your data\n",
    "steps = 1  # You need to determine the appropriate number of steps based on your data\n",
    "new_input_shape = (input_shape[0], steps)\n",
    "encoder_input = Input(shape=input_shape)\n",
    "encoder_reshape = Reshape(new_input_shape)(encoder_input)\n",
    "encoder_conv1 = Conv1D(32, kernel_size=2, activation='relu')(encoder_reshape)\n",
    "encoder_lstm = LSTM(32, activation='relu')(encoder_conv1)\n",
    "latent_dim = 32\n",
    "encoder_output = Dense(latent_dim)(encoder_lstm)\n",
    "\n",
    "encoder_model = Model(encoder_input, encoder_output)\n",
    "\n",
    "# Define the decoder\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "decoder_dense1 = Dense(32, activation='relu')(decoder_input)\n",
    "decoder_reshape = Reshape((1, 32))(decoder_dense1)\n",
    "decoder_conv1 = Conv1D(32, kernel_size=3, activation='relu', padding='same')(decoder_reshape)\n",
    "output_dim = 32\n",
    "decoder_output = Dense(output_dim, activation='sigmoid')(decoder_conv1)\n",
    "\n",
    "decoder_model = Model(decoder_input, decoder_output)\n",
    "\n",
    "# Combine the encoder and decoder into an autoencoder\n",
    "autoencoder_input = Input(shape=input_shape)\n",
    "encoded = encoder_model(autoencoder_input)\n",
    "decoded = decoder_model(encoded)\n",
    "autoencoder_model = Model(autoencoder_input, decoded)\n",
    "\n",
    "# Compile the model\n",
    "autoencoder_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "autoencoder_model.fit(X, Y, epochs=3, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294800d1",
   "metadata": {},
   "source": [
    "# some experiements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf914664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DCC layer (updated version)\n",
    "class DilatedCausalConvolution(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, dilation_rate):\n",
    "        super().__init__()\n",
    "        self.query_conv = layers.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                                        dilation_rate=dilation_rate, padding='causal', activation='relu')\n",
    "        self.key_conv = layers.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                                      dilation_rate=dilation_rate, padding='causal', activation='relu')\n",
    "        self.value_conv = layers.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                                        dilation_rate=dilation_rate, padding='causal', activation='relu')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query = self.query_conv(inputs)\n",
    "        key = self.key_conv(inputs)\n",
    "        value = self.value_conv(inputs)\n",
    "        return query, key, value\n",
    "\n",
    "# Define the Transformer block (updated version)\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, num_heads, d_model):\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.dense_proj = layers.Dense(d_model, activation='relu')\n",
    "\n",
    "    def call(self, query, key, value):\n",
    "        attn_output = self.multi_head_attention(query, key, value)\n",
    "        proj_output = self.dense_proj(attn_output)\n",
    "        return proj_output\n",
    "\n",
    "# Define the encoder using the DCC and Transformer block (updated version)\n",
    "def create_encoder(input_shape, filters, kernel_size, dilation_rate, num_heads, d_model):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    dcc_layer = DilatedCausalConvolution(filters=filters, kernel_size=kernel_size, dilation_rate=dilation_rate)\n",
    "    query, key, value = dcc_layer(inputs)\n",
    "    transformer_block = TransformerBlock(num_heads=num_heads, d_model=d_model)\n",
    "    transformer_output = transformer_block(query, key, value)\n",
    "    model = models.Model(inputs=inputs, outputs=transformer_output)\n",
    "    return model\n",
    "\n",
    "# Define the monotonic decoder (simplified version)\n",
    "def create_monotonic_decoder(input_shape, output_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # The decoder would have constraints to ensure monotonicity, omitted here for simplicity\n",
    "    outputs = layers.Dense(output_shape, activation='sigmoid')(inputs)\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Combine the encoder and decoder to create the full model (updated version)\n",
    "def create_full_model(input_shape, filters, kernel_size, dilation_rate, num_heads, d_model, output_shape):\n",
    "    encoder_inputs = layers.Input(shape=input_shape)\n",
    "    encoder = create_encoder(input_shape, filters, kernel_size, dilation_rate, num_heads, d_model)\n",
    "    encoder_output = encoder(encoder_inputs)\n",
    "    \n",
    "    # Assuming the encoder_output has shape (batch_size, seq_length, d_model)\n",
    "    # If the next layer expects a flat vector, we can flatten the encoder output\n",
    "    # Otherwise, adjust the shape according to the requirements of your decoder\n",
    "    flattened_output = layers.Flatten()(encoder_output)\n",
    "    \n",
    "    decoder = create_monotonic_decoder(flattened_output.shape[1:], output_shape)\n",
    "    decoder_output = decoder(flattened_output)\n",
    "    \n",
    "    full_model = models.Model(inputs=encoder_inputs, outputs=decoder_output)\n",
    "    return full_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e114013f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand dimensions of X to add the channel dimension if needed\n",
    "X_expanded = np.expand_dims(X, axis=-1)  # Shape becomes (99601, 22, 1)\n",
    "\n",
    "# Define model parameters\n",
    "input_shape = (22, 1)  # 22 timesteps and 1 feature per timestep\n",
    "filters = 64\n",
    "kernel_size = 3\n",
    "dilation_rate = 1\n",
    "num_heads = 8\n",
    "d_model = 64\n",
    "output_shape = 1\n",
    "\n",
    "# Create the full model\n",
    "model = create_full_model(input_shape, filters, kernel_size, dilation_rate, num_heads, d_model, output_shape)\n",
    "\n",
    "# Compile the model (define loss and optimizer)\n",
    "model.compile(optimizer='adam', loss='MSE')\n",
    "\n",
    "# Fit the model with the expanded input data\n",
    "model.fit(X_expanded, Y, epochs=10, batch_size=32)\n",
    "\n",
    "# Print the model summary to verify\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3daa500",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
