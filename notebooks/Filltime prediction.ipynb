{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b8b46c54",
      "metadata": {
        "id": "b8b46c54"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import optuna\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.models import Model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers, models, backend, constraints, initializers\n",
        "from keras_self_attention import SeqSelfAttention\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_self_attention"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Z_ByQ2QU8Ia",
        "outputId": "3e2bee8e-e597-4e64-d5f6-4c0e6781f70c"
      },
      "id": "9Z_ByQ2QU8Ia",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras_self_attention in /usr/local/lib/python3.10/dist-packages (0.51.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras_self_attention) (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2cd9ab35",
      "metadata": {
        "id": "2cd9ab35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "4e189503-e7a0-407a-9dd2-fe7def7bf3b9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-2bd066cc5d14>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/truncated_LOB_data_BTC_USD_COINBASE.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/truncated_LOB_data_BTC_USD_COINBASE.csv'"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('/truncated_LOB_data_BTC_USD_COINBASE.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93636ab5",
      "metadata": {
        "id": "93636ab5"
      },
      "outputs": [],
      "source": [
        "def finder_of_fulfilment(df, column):\n",
        "    time, indicator = [], []\n",
        "    for i in range(len(df)):\n",
        "        num = df[column].iloc[i]\n",
        "        arr = df[df[column]>num][column].index\n",
        "        time.append(pd.to_datetime(df['timestamp'].iloc[arr[arr>i][0]]) - pd.to_datetime(df['timestamp'].iloc[i])\n",
        "                    if len(arr[arr>i]) else\n",
        "                    pd.to_datetime('2023-10-03 00:00:00') - pd.to_datetime(df['timestamp'].iloc[i]))\n",
        "        indicator.append(1 if len(arr[arr>i]) else 0)\n",
        "    return time, indicator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "415cf8e4",
      "metadata": {
        "id": "415cf8e4"
      },
      "outputs": [],
      "source": [
        "df['time'], df['indicator'] = finder_of_fulfilment(df, 'ask_prices_0')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30e707e1",
      "metadata": {
        "id": "30e707e1"
      },
      "outputs": [],
      "source": [
        "df['time'] = (df['time']).dt.total_seconds().astype(int)\n",
        "df = df.iloc[0:-140]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdfa8749",
      "metadata": {
        "id": "cdfa8749"
      },
      "outputs": [],
      "source": [
        "df_to_plot = df[['indicator', 'time']]\n",
        "N = len(df_to_plot)\n",
        "S_of_t = []\n",
        "\n",
        "for time in range(max(df['time'])):\n",
        "    num_executed = len(df_to_plot[df_to_plot['time']==time])\n",
        "    num_survived = len(df_to_plot[df_to_plot['time']>time])\n",
        "    prob = num_executed/(num_survived+num_executed)\n",
        "    #print(prob)\n",
        "    S_of_t.append(1-prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7036a649",
      "metadata": {
        "id": "7036a649"
      },
      "outputs": [],
      "source": [
        "S_hat = np.cumprod(S_of_t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cedd69cd",
      "metadata": {
        "id": "cedd69cd"
      },
      "outputs": [],
      "source": [
        "t = np.linspace(0, int(max(df['time'])), int(max(df['time'])))\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(t[:400], S_hat[:400], linewidth=2, label='Level 1, pegged', color='black')\n",
        "plt.xlabel('t (sec.)')\n",
        "plt.ylabel('$\\hat{S}(t)$')\n",
        "plt.title('Survival Probability Over Time')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f85fa37",
      "metadata": {
        "id": "1f85fa37"
      },
      "outputs": [],
      "source": [
        "#Make some features and drop useless columns\n",
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "df['vol_imbalance'] = (df['bid_quantity_0'] - df['ask_quantity_0'])/(df['bid_quantity_0'] + df['ask_quantity_0'])\n",
        "df['microprice'] = ((df['bid_prices_0']*df['bid_quantity_0']+df['ask_prices_0']*df['ask_quantity_0'])\n",
        "                    /(df['bid_quantity_0'] + df['ask_quantity_0']))\n",
        "\n",
        "def logReturns(S):\n",
        "    return np.log(S/S.shift(1))\n",
        "\n",
        "def volFromTimeSeriesPerSecond(prices, timeStamps):\n",
        "    if np.size(prices) <= 2:\n",
        "        raise ValueError('We need more prices than 2')\n",
        "\n",
        "    # Convert timestamp strings to datetime\n",
        "    timeStamps = pd.to_datetime(timeStamps)\n",
        "\n",
        "    x = logReturns(prices)[1:]  # the value at the first index is NaN\n",
        "    xBar = np.mean(x)\n",
        "\n",
        "    # Calculate deltaT in seconds\n",
        "    deltaT = (timeStamps - timeStamps.shift(10)).dt.total_seconds()[1:]  # the value at the first index is NaN\n",
        "    N = np.size(x)\n",
        "\n",
        "    return np.sqrt(np.sum((x - xBar) ** 2 / deltaT) / (N - 1))\n",
        "\n",
        "def volFromTimeSeriesPerHour(prices,timeStamps):\n",
        "    return volFromTimeSeriesPerSecond(prices, timeStamps)*np.sqrt(3600.0)\n",
        "\n",
        "def volFromTimeSeriesPerDay(prices,timeStamps):\n",
        "    return volFromTimeSeriesPerSecond(prices, timeStamps)*np.sqrt(3600.0*24)\n",
        "\n",
        "def volFromTimeSeriesPerYear(prices,timeStamps):\n",
        "    return volFromTimeSeriesPerSecond(prices, timeStamps)*np.sqrt(3600.0*24*365.25)\n",
        "\n",
        "\n",
        "print(\"Vol is %.2f%% per second\" % (100.0*volFromTimeSeriesPerSecond(df['microprice'], df['timestamp'])))\n",
        "print(\"Vol is %.2f%% per hour\" % (100.0*volFromTimeSeriesPerHour(df['microprice'], df['timestamp'])))\n",
        "print(\"Vol is %.2f%% per day\" % (100.0*volFromTimeSeriesPerDay(df['microprice'], df['timestamp'])))\n",
        "print(\"Vol is %.2f%% per year\" % (100.0*volFromTimeSeriesPerYear(df['microprice'], df['timestamp'])))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_volatility(prices, timeStamps):\n",
        "    if len(prices) <= 1:\n",
        "        return np.nan  # Not enough data\n",
        "\n",
        "    x = logReturns(prices)\n",
        "    xBar = np.mean(x)\n",
        "    i=10\n",
        "    deltaT = (timeStamps - timeStamps.shift(i)).dt.total_seconds()\n",
        "    N = len(x)\n",
        "\n",
        "    return np.sqrt(np.nansum((x - xBar) ** 2 / deltaT) / (N - 1))\n",
        "\n",
        "# Define a function to calculate rolling volatility\n",
        "def rolling_volatility(df, window_size):\n",
        "    # Convert timestamp strings to datetime\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "\n",
        "    # Apply the rolling window\n",
        "    rolling_vol = df['microprice'].rolling(window=window_size).apply(\n",
        "        lambda x: calculate_volatility(x, df['timestamp']), raw=False\n",
        "    )\n",
        "\n",
        "    return rolling_vol\n",
        "\n",
        "window_size = 1000\n",
        "\n",
        "# Calculate rolling volatility and add it as a new column\n",
        "df['daily_volatility'] = rolling_volatility(df, window_size)"
      ],
      "metadata": {
        "id": "wX7XS6pRRO6V"
      },
      "id": "wX7XS6pRRO6V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d21ff658",
      "metadata": {
        "id": "d21ff658"
      },
      "outputs": [],
      "source": [
        "target = [] #create target S(execution_time)\n",
        "for i in range(len(df)):\n",
        "    target.append(S_of_t[df['time'].iloc[i]-1])\n",
        "\n",
        "df['target'] = target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3be901f2",
      "metadata": {
        "id": "3be901f2"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "\n",
        "# Plotting the microprice with a blue line\n",
        "plt.plot(df['timestamp'], df['microprice'], color='blue', linewidth=2, label='Microprice')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Microprice')\n",
        "plt.title('Microprice over Time')\n",
        "\n",
        "# Displaying the grid\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4916300",
      "metadata": {
        "id": "d4916300"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plotting the microprice with a blue line\n",
        "plt.plot(df['timestamp'], df['vol_imbalance'].rolling(1000).mean(), color='blue',\n",
        "         linewidth=2, label='Rolling Volume Imbalance')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Volume Imbalance')\n",
        "plt.title('Rolling Volume Imbalance over Time')\n",
        "\n",
        "# Displaying the grid\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a396cb78",
      "metadata": {
        "id": "a396cb78"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plotting the microprice with a blue line\n",
        "plt.plot(df['timestamp'], df['target'].rolling(1000).mean(), color='blue',\n",
        "         linewidth=2, label='Target thing')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Target')\n",
        "plt.title('Rolling TargetTargetTarget')\n",
        "\n",
        "# Displaying the grid\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['daily_volatility'] = df['daily_volatility']*np.sqrt(360000.0*24)*100\n",
        "df"
      ],
      "metadata": {
        "id": "W2IOdcadXq3F"
      },
      "id": "W2IOdcadXq3F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(inplace=True)\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "_B_1CtjkVkVc"
      },
      "id": "_B_1CtjkVkVc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "268a1854",
      "metadata": {
        "id": "268a1854"
      },
      "outputs": [],
      "source": [
        "#df.set_index('timestamp', drop=True, inplace=True)\n",
        "N = 100 # define the number of previous observation included\n",
        "\n",
        "X = df.drop(columns=['timestamp', 'time', 'indicator', 'target'])\n",
        "Y = df['indicator'].iloc[N:len(X)//10].values\n",
        "print(Y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Y = Y[1:]"
      ],
      "metadata": {
        "id": "FzWP-HRVYJK7"
      },
      "id": "FzWP-HRVYJK7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7973c179",
      "metadata": {
        "id": "7973c179"
      },
      "outputs": [],
      "source": [
        "tensor_slices = []\n",
        "\n",
        "for i in range(N, len(X)//10):\n",
        "    start_index = max(0, i - N)\n",
        "    slice_df = X.iloc[start_index:i+1]\n",
        "\n",
        "    tensor_slice = tf.convert_to_tensor(slice_df, dtype=tf.float32)\n",
        "    tensor_slices.append(tensor_slice)\n",
        "\n",
        "X = tf.stack(tensor_slices)\n",
        "\n",
        "print(X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (N+1, N+1, 1)\n",
        "encoder_input = layers.Input(shape=input_shape)\n",
        "\n",
        "# Encoder\n",
        "encoder_conv1 = layers.Conv1D(4, kernel_size=2, activation='relu', padding='casual')(encoder_input)\n",
        "encoder_conv2 = layers.Conv1D(4, kernel_size=2, activation='relu', padding='casual')(encoder_conv1)\n",
        "encoder_flatten = layers.Flatten()(encoder_conv2)\n",
        "latent_dim = 4\n",
        "encoder_output = layers.Dense(latent_dim, activation='relu')(encoder_flatten)\n",
        "\n",
        "encoder_model = Model(encoder_input, encoder_output)\n",
        "\n",
        "# Decoder\n",
        "decoder_input = layers.Input(shape=(latent_dim,))\n",
        "# Compute the shape after the last conv layer before flattening\n",
        "shape_before_flattening = encoder_conv2.shape[1:]\n",
        "decoder_dense1 = layers.Dense(np.prod(shape_before_flattening), activation='relu')(decoder_input)\n",
        "decoder_reshape = layers.Reshape(shape_before_flattening)(decoder_dense1)\n",
        "decoder_conv1 = layers.Conv2DTranspose(4, kernel_size=2, activation='relu', padding='casual')(decoder_reshape)\n",
        "decoder_conv2 = layers.Conv2DTranspose(4, kernel_size=2, activation='relu', padding='casual')(decoder_conv1)\n",
        "decoder_flatten = layers.Flatten()(decoder_conv2)\n",
        "decoder_output = layers.Dense(1, activation='relu')(decoder_flatten)  # Adjust based on the shape of Y\n",
        "\n",
        "decoder_model = Model(decoder_input, decoder_output)\n",
        "\n",
        "# Autoencoder\n",
        "autoencoder_input = layers.Input(shape=input_shape)\n",
        "encoded = encoder_model(autoencoder_input)\n",
        "decoded = decoder_model(encoded)\n",
        "autoencoder_model = Model(autoencoder_input, decoded)\n",
        "\n",
        "autoencoder_model.compile(optimizer='adam', loss='MAE')\n",
        "\n",
        "# Reshape X to include the channel dimension\n",
        "X_reshaped = np.expand_dims(X, axis=-1)\n",
        "\n",
        "# Fit the model to X and Y\n",
        "autoencoder_model.fit(X_reshaped, Y, epochs=3, batch_size=16)"
      ],
      "metadata": {
        "id": "tuxraJrsiKov"
      },
      "id": "tuxraJrsiKov",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "44fe5e5e",
      "metadata": {
        "id": "44fe5e5e"
      },
      "source": [
        "# Simplified Staff (LSTM instead of Att)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred = autoencoder_model.predict(X)"
      ],
      "metadata": {
        "id": "3yKoqSmWhnf9"
      },
      "id": "3yKoqSmWhnf9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(Y_pred)"
      ],
      "metadata": {
        "id": "XRS8Ru7HkMZk"
      },
      "id": "XRS8Ru7HkMZk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "294800d1",
      "metadata": {
        "id": "294800d1"
      },
      "source": [
        "# Some experiements (model is almost ready monotonicity change is needed or S(t) to f(t) transition)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PMu0US83oEG6",
      "metadata": {
        "id": "PMu0US83oEG6"
      },
      "outputs": [],
      "source": [
        "plt.plot(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de282651",
      "metadata": {
        "id": "de282651"
      },
      "outputs": [],
      "source": [
        "# Define the DCC layer\n",
        "# Redefine the DCC layer to include ReLU activation and causal padding\n",
        "\n",
        "# class DilatedCausalConvolution(layers.Layer):\n",
        "#     def __init__(self, filters, kernel_size, dilation_rate):\n",
        "#         super().__init__()\n",
        "#         self.query_conv = layers.Conv1D(filters=filters, kernel_size=kernel_size,\n",
        "#                                         dilation_rate=dilation_rate, padding='same', activation='tanh')\n",
        "#         self.key_conv = layers.Conv1D(filters=filters, kernel_size=kernel_size,\n",
        "#                                       dilation_rate=dilation_rate, padding='same', activation='tanh')\n",
        "#         self.value_conv = layers.Conv1D(filters=filters, kernel_size=kernel_size,\n",
        "#                                         dilation_rate=dilation_rate, padding='same', activation='tanh')\n",
        "\n",
        "#     def call(self, inputs):\n",
        "#         query = self.query_conv(inputs)\n",
        "#         key = self.key_conv(inputs)\n",
        "#         value = self.value_conv(inputs)\n",
        "#         return query, key, value\n",
        "\n",
        "# # Define the Transformer block\n",
        "# class TransformerBlock(layers.Layer):\n",
        "#     def __init__(self, num_heads, d_model):\n",
        "#         super().__init__()\n",
        "#         self.multi_head_attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "\n",
        "#     def call(self, query, key, value):\n",
        "#         attn_output = self.multi_head_attention(query, key, value)\n",
        "#         proj_output = tf.concat(attn_output, axis=0)\n",
        "#         return proj_output\n",
        "\n",
        "# # Define the encoder using the DCC and Transformer block\n",
        "# def create_encoder(input_shape, filters, kernel_size, dilation_rate, num_heads, d_model):\n",
        "#     inputs = layers.Input(shape=input_shape)\n",
        "#     dcc_layer = DilatedCausalConvolution(filters=filters, kernel_size=kernel_size, dilation_rate=dilation_rate)\n",
        "#     query, key, value = dcc_layer(inputs)\n",
        "#     transformer_block = TransformerBlock(num_heads=num_heads, d_model=d_model)\n",
        "#     transformer_output = transformer_block(query, key, value)\n",
        "#     model = models.Model(inputs=inputs, outputs=transformer_output)\n",
        "#     return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# new part\n",
        "class DilatedCausalConvolution(layers.Layer):\n",
        "    def __init__(self, filters, kernel_size, dilation_rate):\n",
        "        super().__init__()\n",
        "        self.conv = layers.Conv1D(filters=filters, kernel_size=kernel_size,\n",
        "                                  dilation_rate=dilation_rate, padding='causal', activation='relu')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.conv(inputs)\n",
        "\n",
        "# Define the Transformer block (assuming it remains the same)\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, num_heads, d_model):\n",
        "        super().__init__()\n",
        "        self.multi_head_attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "\n",
        "    def call(self, query, key, value):\n",
        "        attn_output = self.multi_head_attention(query, key, value)\n",
        "        proj_output = tf.concat(attn_output, axis=0)\n",
        "        return proj_output\n",
        "\n",
        "# Define the new encoder using the updated DCC\n",
        "def create_encoder(input_shape, d_model, filters, kernel_size):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Updated convolutional layers with specified dilation rates\n",
        "    dilation_rates = [1, 2, 4, 8, 16]\n",
        "    x = inputs\n",
        "    for rate in dilation_rates:\n",
        "        x = DilatedCausalConvolution(filters=filters, kernel_size=kernel_size, dilation_rate=rate)(x)\n",
        "\n",
        "    normalized = layers.LayerNormalization()(x)\n",
        "\n",
        "    # Temporal encoding (assuming a simple range-based encoding for illustration)\n",
        "    temporal_encoding = tf.range(d_model, dtype=tf.float32)\n",
        "    temporal_encoding = layers.Reshape((1, d_model))(temporal_encoding)\n",
        "    temporal_encoding = tf.tile(temporal_encoding, [tf.shape(normalized)[0], N+1, 1])\n",
        "\n",
        "    # Concatenating with temporal encoding\n",
        "    x = tf.concat([normalized, temporal_encoding], axis=2)\n",
        "\n",
        "    # Adjust the shape [100, 15] if necessary (e.g., using a Dense layer)\n",
        "    x = layers.Dense(15)(x)\n",
        "\n",
        "    # Multi-head self-attention\n",
        "    transformer_block = TransformerBlock(num_heads=num_heads, d_model=d_model)\n",
        "    transformer_output = transformer_block(x, x, x)\n",
        "\n",
        "    model = models.Model(inputs=inputs, outputs=transformer_output)\n",
        "    return model\n",
        "    transformer_block = TransformerBlock(num_heads=3, d_model=d_model)\n",
        "    transformer_output = transformer_block(x, x, x)\n",
        "\n",
        "    model = models.Model(inputs=inputs, outputs=transformer_output)\n",
        "    return model\n",
        "# Define the monotonic decoder\n",
        "def monotonic_constraint(weight_matrix):\n",
        "    return tf.where(weight_matrix < 0, tf.zeros_like(weight_matrix), weight_matrix)\n",
        "\n",
        "class CustomDense(layers.Layer):\n",
        "    def __init__(self, units, activation='sigmoid', **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.activation = tf.keras.activations.get(activation)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        last_dim = input_shape[-1] if isinstance(input_shape[-1], int) else input_shape[-1].value\n",
        "        self.kernel = self.add_weight(name='kernel', shape=(last_dim, self.units),\n",
        "                                      initializer='zeros', constraint=monotonic_constraint, trainable=True)\n",
        "        self.bias = self.add_weight(name='bias', shape=(self.units,), initializer='zeros', trainable=False)\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.activation(tf.matmul(inputs, self.kernel) + self.bias)\n",
        "\n",
        "class CustomDecoder(layers.Layer):\n",
        "    def __init__(self, input_dim, units, activation='relu', num_layers=5, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_layers = num_layers\n",
        "        self.dense_layers = [CustomDense(units, activation=activation) for _ in range(num_layers)]\n",
        "        self.dense_layers.append(CustomDense(units, activation='sigmoid'))\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.dense_layers[0].build((None, self.input_dim))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = inputs\n",
        "        for layer in self.dense_layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "# Combine the encoder and decoder to create the full model\n",
        "def create_full_model(input_shape, filters, kernel_size, num_heads, d_model, output_shape, units):\n",
        "    encoder_inputs = layers.Input(shape=input_shape)\n",
        "    encoder = create_encoder(input_shape, d_model, filters, kernel_size)\n",
        "    encoder_output = encoder(encoder_inputs)\n",
        "\n",
        "    encoder_output_dim = encoder_output.shape[-1]\n",
        "\n",
        "    decoder = CustomDecoder(input_dim=encoder_output_dim, units=units, activation='relu')\n",
        "    decoder_output = decoder(encoder_output)\n",
        "    flat_output = tf.keras.layers.Flatten()(decoder_output)\n",
        "    final_output = tf.keras.layers.Dense(output_shape)(flat_output)\n",
        "    full_model = models.Model(inputs=encoder_inputs, outputs=final_output)\n",
        "\n",
        "    return full_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model parameters\n",
        "input_shape = (N+1, N+1)  # Adjust based on your actual data dimensions\n",
        "num_heads = 3\n",
        "output_shape = 1"
      ],
      "metadata": {
        "id": "p4P8ht8jyePD"
      },
      "id": "p4P8ht8jyePD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "biUDVozSdhVw",
      "metadata": {
        "id": "biUDVozSdhVw"
      },
      "outputs": [],
      "source": [
        "# Optuna objective function\n",
        "def objective(trial):\n",
        "    # Suggest values for the hyperparameters\n",
        "    filters = trial.suggest_categorical('filters', [2, 4, 8, 16])\n",
        "    units = trial.suggest_categorical('units', [1, 4, 8, 16, 32])\n",
        "    kernel_size = trial.suggest_categorical('kernel_size', [3, 5, 7])\n",
        "    #dilation_rate = trial.suggest_categorical('dilation_rate', [1, 2, 3, 4])\n",
        "    d_model = trial.suggest_categorical('d_model', [1, 2, 4, 8, 32])\n",
        "    batch = trial.suggest_categorical('batch', [4, 8, 16, 32])\n",
        "\n",
        "    # Create the model using the suggested values\n",
        "    model = create_full_model(input_shape, filters, kernel_size, num_heads, d_model, output_shape, units)\n",
        "\n",
        "    # Compile the model (same as your existing code)\n",
        "    initial_learning_rate = 0.01\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate, decay_steps=1000, decay_rate=0.96, staircase=True)\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=lr_schedule),\n",
        "                  loss=tf.keras.losses.MeanAbsoluteError())\n",
        "\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((X, Y)).batch(batch_size = batch)\n",
        "\n",
        "    callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
        "    history = model.fit(train_ds, epochs=10, callbacks=[callback], verbose=1)\n",
        "\n",
        "    val_loss = min(history.history['loss'])\n",
        "    return val_loss\n",
        "\n",
        "# Optuna study\n",
        "study = optuna.create_study(direction='minimize')  # 'minimize' if lower loss is better, 'maximize' otherwise\n",
        "study.optimize(objective, n_trials=5)  # Adjust the number of trials\n",
        "\n",
        "# Print the optimal hyperparameters\n",
        "print('Best trial:')\n",
        "trial = study.best_trial\n",
        "print(f'  Value: {trial.value}')\n",
        "print('  Params: ')\n",
        "for key, value in trial.params.items():\n",
        "    print(f'    {key}: {value}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model parameters\n",
        "input_shape = (N+1, 23)\n",
        "filters = 2\n",
        "units = 32\n",
        "kernel_size = 7\n",
        "dilation_rate = 3\n",
        "num_heads = 3\n",
        "d_model = 1\n",
        "output_shape = 1\n",
        "batch = 4 # to determine\n",
        "# Create the model\n",
        "model = create_full_model(input_shape, filters, kernel_size, num_heads, d_model, output_shape, units)\n",
        "\n",
        "# Create TensorFlow dataset\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X, Y)).batch(batch_size=batch)\n",
        "\n",
        "# Compile the model\n",
        "initial_learning_rate = 0.01\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate, decay_steps=1000, decay_rate=0.96, staircase=True)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
        "              loss=tf.keras.losses.MeanAbsoluteError())\n",
        "\n",
        "# Train the model\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
        "model.fit(train_ds, epochs=100, callbacks=[callback])"
      ],
      "metadata": {
        "id": "wgPEvxifbRzO"
      },
      "id": "wgPEvxifbRzO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred = model.predict(X)"
      ],
      "metadata": {
        "id": "Fo-pZrxIdoAa"
      },
      "id": "Fo-pZrxIdoAa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred"
      ],
      "metadata": {
        "id": "r-WXXEZlhRlI"
      },
      "id": "r-WXXEZlhRlI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QgQPyJhahThM"
      },
      "id": "QgQPyJhahThM",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}